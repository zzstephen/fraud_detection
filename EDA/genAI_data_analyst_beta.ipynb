{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e94fee4-bfbb-431e-a142-10de8d04a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import time\n",
    "sys.path.append('../utilities')\n",
    "from basic_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8294a70-1e08-443b-b400-d482131385df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../../api_keys/openai_api.txt\", \"r\") as password_file:  # Open in binary read mode\n",
    "    \n",
    "    password = password_file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeb856c-1897-4181-9737-ad86d88b25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef38b1d-c482-4f31-8c17-be4d91da2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data_path = '../../../data/processed_data/train_test_sample.csv'\n",
    "\n",
    "train_test_data_txt = '../../../data/processed_data/train_test_sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f163841f-8ddf-4153-a846-df2d468febac",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_txt(train_test_data_path, train_test_data_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74f2e08-4266-4d71-9818-6860c24a9ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 written to ../../../data/processed_data/chunk_1.txt\n",
      "Chunk 2 written to ../../../data/processed_data/chunk_2.txt\n",
      "Chunk 3 written to ../../../data/processed_data/chunk_3.txt\n",
      "Chunk 4 written to ../../../data/processed_data/chunk_4.txt\n",
      "Chunk 5 written to ../../../data/processed_data/chunk_5.txt\n",
      "Chunk 6 written to ../../../data/processed_data/chunk_6.txt\n",
      "Chunk 7 written to ../../../data/processed_data/chunk_7.txt\n",
      "Chunk 8 written to ../../../data/processed_data/chunk_8.txt\n",
      "Chunk 9 written to ../../../data/processed_data/chunk_9.txt\n",
      "Chunk 10 written to ../../../data/processed_data/chunk_10.txt\n",
      "Chunk 11 written to ../../../data/processed_data/chunk_11.txt\n",
      "Chunk 12 written to ../../../data/processed_data/chunk_12.txt\n",
      "Chunk 13 written to ../../../data/processed_data/chunk_13.txt\n",
      "Chunk 14 written to ../../../data/processed_data/chunk_14.txt\n",
      "Chunk 15 written to ../../../data/processed_data/chunk_15.txt\n",
      "Chunk 16 written to ../../../data/processed_data/chunk_16.txt\n",
      "Chunk 17 written to ../../../data/processed_data/chunk_17.txt\n",
      "Chunk 18 written to ../../../data/processed_data/chunk_18.txt\n"
     ]
    }
   ],
   "source": [
    "split_file(train_test_data_txt,'../../../data/processed_data' ,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff1c2f02-ac2d-4a2f-99d9-c06c9593e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files = [f\"../../../data/processed_data/chunk_{i+1}.txt\" for i in range(18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35795fed-2a56-4187-8a5a-d6eb2331a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a seasoned data scientist and engineer\"}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c47810-62f0-4705-b42a-fe7d59617a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #chatbot\n",
    "\n",
    "# while True:\n",
    "\n",
    "#     user_input = input(\"Ask me anything. Or type quit to disengage.\")\n",
    "\n",
    "#     if user_input == 'quit':\n",
    "#         break\n",
    "#     else:\n",
    "#         messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "#         response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\", # Choose your desired model\n",
    "#         messages=messages)\n",
    "\n",
    "#         # pdb.set_trace()\n",
    "\n",
    "#         response_context = response.choices[0].message.content\n",
    "#         print(response_context)\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": response_context})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48baf500-f858-4128-824c-7f0b514c5f7e",
   "metadata": {},
   "source": [
    "### Data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ba0a95-188d-4a6b-8ded-3a9eb6f70f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "    name=\"store_csv_for_data_analyst_assistant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95856c3-b3dd-4aea-bc64-e9ef4c2ceed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/processed_data/chunk_1.txt upload status: completed\n",
      "../../../data/processed_data/chunk_2.txt upload status: completed\n",
      "../../../data/processed_data/chunk_3.txt upload status: completed\n",
      "../../../data/processed_data/chunk_4.txt upload status: completed\n",
      "../../../data/processed_data/chunk_5.txt upload status: completed\n",
      "../../../data/processed_data/chunk_6.txt upload status: completed\n",
      "../../../data/processed_data/chunk_7.txt upload status: completed\n",
      "../../../data/processed_data/chunk_8.txt upload status: completed\n",
      "../../../data/processed_data/chunk_9.txt upload status: completed\n",
      "../../../data/processed_data/chunk_10.txt upload status: completed\n",
      "../../../data/processed_data/chunk_11.txt upload status: completed\n",
      "../../../data/processed_data/chunk_12.txt upload status: completed\n",
      "../../../data/processed_data/chunk_13.txt upload status: completed\n",
      "../../../data/processed_data/chunk_14.txt upload status: completed\n",
      "../../../data/processed_data/chunk_15.txt upload status: completed\n",
      "../../../data/processed_data/chunk_16.txt upload status: completed\n",
      "../../../data/processed_data/chunk_17.txt upload status: completed\n",
      "../../../data/processed_data/chunk_18.txt upload status: completed\n"
     ]
    }
   ],
   "source": [
    "resource_file_id = []\n",
    "\n",
    "for chunk in upload_files:\n",
    "\n",
    "    file = client.files.create(\n",
    "      file=open(chunk, \"rb\"),\n",
    "      purpose='assistants'\n",
    "    )\n",
    "    \n",
    "    vector_store_file = client.vector_stores.files.create_and_poll(\n",
    "        vector_store_id=vector_store.id,\n",
    "        file_id=file.id,\n",
    "    )\n",
    "    \n",
    "    resource_file_id.append(file.id)\n",
    "    \n",
    "    print(f\"{chunk} upload status: {vector_store_file.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e86d51-bc5c-4e9e-9ec6-3c0ac2d93002",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"data_analysis_assistant\",\n",
    "    instructions=\"You are an expert in data analysis. You help me summarize data\",\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"code_interpreter\", 'type': 'file_search'}],\n",
    "    tool_resources={\n",
    "    \"code_interpreter\": {\"file_ids\": resource_file_id}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c475027-21fb-41db-be2a-04acf31d1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_ASSISTANT_ID = assistant.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab1127e-0ee1-4254-a80e-95340890703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6614e402-e882-4b64-a256-ab50ef36a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9086d283-d8b4-42a8-8967-c15159b73907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(messages):\n",
    "    print(\"# Messages\")\n",
    "    for m in messages:\n",
    "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4cacfdf-310f-4a14-a8d6-665dba2f132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I am your data analyst assistant. I can summarize data for you. Type quit to exit can you tell me how many rows and columns in the resource that I uploaded to you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "assistant: I was unable to determine the number of rows and columns directly from the uploaded files. To help with this, I'd need to extract and analyze the data from each file individually. Let me proceed to read through them to get this information.\n",
      "assistant: I will now proceed to check each file one by one to provide you with the number of rows and columns they contain.\n",
      "assistant: It seems that I'm unable to automatically extract the number of rows and columns directly from the files at the moment. You can help by converting them into a more straightforward text or spreadsheet format, and I can read them directly. Alternatively, if you can share a smaller sample or tell me their file formats, I can guide you on how to find this information on your own. \n",
      "\n",
      "If these are spreadsheets or CSV files, here's a simple way to check:\n",
      "\n",
      "1. **Excel or Google Sheets**: Open the file in Excel or Google Sheets. The number of columns is visible along the top (A, B, C, etc.), and you can scroll down to the bottom to see the number of rows.\n",
      "\n",
      "2. **CSV Files**: Open the file in a text editor. Rows are separated by new lines, and columns by commas. \n",
      "\n",
      "3. **Data Analysis Tools**: You can use data analysis or programming tools like Python (pandas library) or R to load the file and check its `shape`.\n",
      "\n",
      "Let me know if you'd like any specific guidance or if there's another way I can help!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I am your data analyst assistant. I can summarize data for you. Type quit to exit I uploaded 18 txt files. Concatenate and read through them\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "assistant: Let's proceed by concatenating the content of the uploaded text files to analyze them for the number of rows and columns. I'll gather the contents of these files incrementally.\n",
      "assistant: I seem to be experiencing issues with accessing the content of the uploaded text files directly. I recommend the following options to determine the number of rows and columns quickly:\n",
      "\n",
      "- **Using a script**: You can write a simple script in Python or any scripting language to read and concatenate these text files. Here's an example Python snippet using the `pandas` library:\n",
      "\n",
      "  ```python\n",
      "  import pandas as pd\n",
      "  import os\n",
      "\n",
      "  # Directory where files are stored\n",
      "  path = 'your_directory_path_here'\n",
      "\n",
      "  # List all files\n",
      "  all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.txt')]\n",
      "\n",
      "  # Read each file and concatenate\n",
      "  all_data = pd.concat((pd.read_csv(f, sep='\\t') for f in all_files))\n",
      "\n",
      "  # Get shape\n",
      "  print(all_data.shape)  # prints (number_of_rows, number_of_columns)\n",
      "  ```\n",
      "\n",
      "  Replace `'your_directory_path_here'` with the path to your files.\n",
      "\n",
      "- **Manual count**: If the files are small, you could open them with a text editor and manually count the rows and columns by checking a sample line for the number of delimiters (like commas or tabs) to understand how many columns there are.\n",
      "\n",
      "Please let me know if you prefer another approach, or if there's anything else I can assist you with!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I am your data analyst assistant. I can summarize data for you. Type quit to exit do you have the txt files I uploaded to you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "assistant: Yes, you uploaded a series of text files. Since there are multiple files, to analyze them collectively, I'd need to access and read their contents individually or in batches to concatenate them. I can process them sequentially to give you the consolidated data overview. Let me attempt to fetch and read the contents of each file. \n",
      "\n",
      "I'll go ahead and retrieve and concatenate the contents of the text files now.\n",
      "assistant: It seems I cannot directly access the content of the text files at the moment. However, you can upload them again or a representative file for me to check its content and suggest specific steps to concatenate and analyze these files using scripts or data analysis tools.\n",
      "\n",
      "If you're able to provide more context about the structure of these files, such as whether they are table-like with consistent delimiters, I can offer more targeted advice or script templates to help you process them.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "I am your data analyst assistant. I can summarize data for you. Type quit to exit quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    user_input = input(\"I am your data analyst assistant. I can summarize data for you. Type quit to exit\")\n",
    "\n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "    else:\n",
    "        message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"user\", content=user_input)\n",
    "\n",
    "        run = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id)\n",
    "\n",
    "        wait_on_run(run, thread)\n",
    "\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id, order=\"asc\", after=message.id)\n",
    "        \n",
    "        pretty_print(messages)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca5590-1bfb-40cd-ba6c-95417ab6019d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816da650-a0c0-4968-aad2-24d75d2dcc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d83cc-bbaf-4866-b3cb-f3c91ab4229b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77392804-df8a-4aac-b9ee-15ec82eb579b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae9cf98-4928-40d4-b57e-aad94429070f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b9856-fd72-40c6-9546-23db4578e889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e2746-a765-4791-b6c9-1c8aaf570ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1873d10-fc84-45c8-85ec-4ea83e71e80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcff40-b9c7-4bb1-b5ce-89e3015792be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a16d16-23eb-4543-bc9a-9ec04ee627ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
